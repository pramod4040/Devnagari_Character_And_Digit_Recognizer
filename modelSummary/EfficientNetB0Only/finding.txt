def create_efficientnet_model(input_shape, num_classes):
    """Create simplified EfficientNetB0-based model for classification with input reshaping"""
    # Load pre-trained EfficientNetB0 without top layers
    base_model = EfficientNetB0(
        weights='imagenet',
        include_top=False,
        input_shape=efficientnet_input_shape  # EfficientNet expects 224x224
    )
    
    # Fine-tuning strategy: freeze early layers, train later layers
    base_model.trainable = True
    
    # Fine-tune the last 1/3 of the model (freeze first 2/3)
    fine_tune_at = len(base_model.layers) * 2 // 3  # Index where fine-tuning starts
    
    # Freeze the earlier layers (first 2/3 of the model)
    for layer in base_model.layers[:fine_tune_at]:
        layer.trainable = False
    
    # Build the complete model with input reshaping
    inputs = Input(shape=input_shape, name='input_layer')  # 32x32x3
    resized_input = layers.Resizing(224, 224)(inputs)
    
    # Apply the base model
    x = base_model(resized_input, training=False)
    
    # Minimal classification head
    x = GlobalAveragePooling2D(name='global_average_pooling')(x)
    x = Dropout(0.2, name='dropout')(x)
    
    # Direct output layer - EfficientNetB0 features -> 46 classes
    outputs = Dense(num_classes, activation='softmax', name='output_layer')(x)
    
    # Create the model
    model = models.Model(inputs, outputs, name='EfficientNetB0_Classifier')
    
    return model




- this is the function EfficientNetB0 is trained under, we are using pretrained weight and making one third of the layers trainable